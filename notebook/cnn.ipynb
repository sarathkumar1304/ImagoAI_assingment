{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sarath_kumar/ImagoAI'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir(\"../\")\n",
    "# %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.86843952 0.05101268]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/TASK-ML-INTERN.csv\")  # Replace with actual dataset\n",
    "df.drop(\"hsi_id\",axis=1,inplace = True)\n",
    "\n",
    "\n",
    "# Remove outliers using IQR\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Separate features and target\n",
    "X = df.iloc[:, :-1].values  # All except last column\n",
    "y = df.iloc[:, -1].values   # Last column (target variable)\n",
    "\n",
    "# Normalize input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2,whiten=True,svd_solver=\"randomized\")\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Variance Explained\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Reshape data for CNN and LSTM (samples, time steps, features)\n",
    "X_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "\n",
    "# Train-Test Split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 448, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarath_kumar/ImagoAI/myenv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 93ms/step - loss: 1188479.2500 - mae: 658.9849 - val_loss: 888972.1875 - val_mae: 564.0150\n",
      "Epoch 2/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 733957.0000 - mae: 539.8466 - val_loss: 750207.3125 - val_mae: 508.6580\n",
      "Epoch 3/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 1029242.0625 - mae: 689.2980 - val_loss: 741739.0625 - val_mae: 522.3793\n",
      "Epoch 4/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 701180.3750 - mae: 588.5511 - val_loss: 707154.3125 - val_mae: 541.9195\n",
      "Epoch 5/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 646483.6250 - mae: 578.6796 - val_loss: 705272.6250 - val_mae: 565.2292\n",
      "Epoch 6/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 725181.5625 - mae: 616.9727 - val_loss: 728392.3125 - val_mae: 623.1910\n",
      "Epoch 7/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 663725.4375 - mae: 598.7700 - val_loss: 663419.1875 - val_mae: 541.9927\n",
      "Epoch 8/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 601552.5625 - mae: 522.1644 - val_loss: 673011.0625 - val_mae: 573.1619\n",
      "Epoch 9/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 605677.3750 - mae: 558.7065 - val_loss: 642560.4375 - val_mae: 508.6448\n",
      "Epoch 10/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 508553.1250 - mae: 482.7098 - val_loss: 650561.3125 - val_mae: 545.4963\n",
      "Epoch 11/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 692508.8750 - mae: 586.2415 - val_loss: 601436.8750 - val_mae: 514.4330\n",
      "Epoch 12/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 474184.4375 - mae: 491.4168 - val_loss: 617948.5000 - val_mae: 540.0986\n",
      "Epoch 13/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 216530.3594 - mae: 329.0327 - val_loss: 567829.3750 - val_mae: 504.6931\n",
      "Epoch 14/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 326651.4688 - mae: 435.5154 - val_loss: 651485.1875 - val_mae: 507.3839\n",
      "Epoch 15/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - loss: 293799.8125 - mae: 378.4619 - val_loss: 598875.9375 - val_mae: 507.0209\n",
      "Epoch 16/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - loss: 173660.4688 - mae: 288.1191 - val_loss: 590439.3125 - val_mae: 498.5705\n",
      "Epoch 17/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 156303.5312 - mae: 277.7299 - val_loss: 550218.5625 - val_mae: 498.6173\n",
      "Epoch 18/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 115656.5391 - mae: 250.7225 - val_loss: 533125.4375 - val_mae: 489.9269\n",
      "Epoch 19/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 71892.5547 - mae: 188.0856 - val_loss: 524616.3750 - val_mae: 498.5632\n",
      "Epoch 20/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 73779.5312 - mae: 197.1765 - val_loss: 529973.0625 - val_mae: 477.2346\n",
      "Epoch 21/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 120856.0000 - mae: 226.9693 - val_loss: 549200.9375 - val_mae: 532.6610\n",
      "Epoch 22/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 84812.8906 - mae: 185.4812 - val_loss: 506791.9062 - val_mae: 524.8746\n",
      "Epoch 23/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 90382.4219 - mae: 192.1997 - val_loss: 601703.5000 - val_mae: 554.2633\n",
      "Epoch 24/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 126021.7188 - mae: 206.9445 - val_loss: 494922.5625 - val_mae: 497.8959\n",
      "Epoch 25/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 76003.6250 - mae: 184.4619 - val_loss: 583611.8750 - val_mae: 565.2325\n",
      "Epoch 26/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 91570.3359 - mae: 189.2139 - val_loss: 554911.9375 - val_mae: 560.8075\n",
      "Epoch 27/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 71473.1562 - mae: 179.3396 - val_loss: 543240.3750 - val_mae: 551.1045\n",
      "Epoch 28/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 72523.9062 - mae: 176.9572 - val_loss: 549261.3125 - val_mae: 563.8243\n",
      "Epoch 29/50\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 47647.7148 - mae: 145.5428 - val_loss: 538065.0000 - val_mae: 573.3018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fd94843a8c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build Optimized CNN Model for Numerical Data\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train.shape[1], 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(1, activation='linear')  # Regression output\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "cnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "# Implement Early Stopping to Prevent Underfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='min')\n",
    "\n",
    "# Train Model\n",
    "cnn_model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fd924ff03a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 175ms/stepWARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fd924ff03a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "MAE: 497.8959\n",
      "RMSE: 703.5058\n",
      "R² Score: 0.1743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Predictions\n",
    "y_pred_cnn = cnn_model.predict(X_test)\n",
    "# y_pred_lstm = lstm_model.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "evaluate_model(y_test, y_pred_cnn)\n",
    "# evaluate_model(y_test, y_pred_lstm, \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Metrics\n",
    "**Mean Absolute Error (MAE: 497.8959):**\n",
    " - On average, the model's predictions are off by 497.89 units from the actual values.\n",
    "\n",
    "**Root Mean Squared Error (RMSE: 703.5058):**\n",
    " - The model has a relatively high RMSE, indicating that large errors exist in predictions.\n",
    "\n",
    "\n",
    "**R² Score (0.1743):**\n",
    " - The model explains only 17% of the variance in the target variable, which suggests poor predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/TASK-ML-INTERN.csv\")  # Replace with actual dataset\n",
    "df.drop(\"hsi_id\", axis=1, inplace=True)\n",
    "\n",
    "# Remove outliers using IQR\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Separate features and target\n",
    "X = df.iloc[:, :-1].values  # All except last column\n",
    "y = df.iloc[:, -1].values   # Last column (target variable)\n",
    "\n",
    "# Normalize input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Reshape data for CNN and LSTM (samples, time steps, features)\n",
    "X_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "\n",
    "# Train-Test Split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarath_kumar/ImagoAI/myenv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 109ms/step - loss: 1021472.8750 - mae: 594.8243 - val_loss: 897742.6875 - val_mae: 568.6235\n",
      "Epoch 2/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 582689.5625 - mae: 522.1036 - val_loss: 719694.5625 - val_mae: 502.8279\n",
      "Epoch 3/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 751694.1250 - mae: 629.3843 - val_loss: 682398.3750 - val_mae: 509.4200\n",
      "Epoch 4/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 879750.3125 - mae: 622.3510 - val_loss: 663486.2500 - val_mae: 508.3220\n",
      "Epoch 5/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 722706.7500 - mae: 582.5474 - val_loss: 631469.5000 - val_mae: 528.9585\n",
      "Epoch 6/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 647799.6250 - mae: 576.0213 - val_loss: 624882.3750 - val_mae: 521.6489\n",
      "Epoch 7/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 658533.6250 - mae: 570.1211 - val_loss: 624221.5625 - val_mae: 539.9049\n",
      "Epoch 8/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 703962.3125 - mae: 613.0764 - val_loss: 607480.1875 - val_mae: 530.5802\n",
      "Epoch 9/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 548379.2500 - mae: 506.6804 - val_loss: 612743.5625 - val_mae: 539.0749\n",
      "Epoch 10/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 505666.2812 - mae: 522.8667 - val_loss: 603027.4375 - val_mae: 525.6542\n",
      "Epoch 11/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 390905.0938 - mae: 435.2102 - val_loss: 618002.5000 - val_mae: 504.6594\n",
      "Epoch 12/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 399166.8750 - mae: 451.9074 - val_loss: 593155.1250 - val_mae: 506.3147\n",
      "Epoch 13/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 234147.5156 - mae: 355.4667 - val_loss: 600700.4375 - val_mae: 502.8320\n",
      "Epoch 14/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 201219.3281 - mae: 311.0501 - val_loss: 566449.0000 - val_mae: 514.5278\n",
      "Epoch 15/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 128880.6875 - mae: 262.3976 - val_loss: 612031.5000 - val_mae: 513.7431\n",
      "Epoch 16/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - loss: 129112.5234 - mae: 262.2979 - val_loss: 705998.3750 - val_mae: 544.2310\n",
      "Epoch 17/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 149662.3125 - mae: 276.1902 - val_loss: 618941.1875 - val_mae: 529.5504\n",
      "Epoch 18/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 94558.4531 - mae: 222.7456 - val_loss: 598735.5625 - val_mae: 521.4332\n",
      "Epoch 19/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 111571.6484 - mae: 215.1001 - val_loss: 578157.3750 - val_mae: 512.5585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fd968ad3d00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Build Optimized CNN Model for Numerical Data\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train.shape[1], 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(1, activation='linear')  # Regression output\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "cnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "# Implement Early Stopping to Prevent Underfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='min')\n",
    "\n",
    "# Train Model\n",
    "cnn_model.fit(X_train, y_train, epochs=20, batch_size=10, validation_data=(X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "MAE: 514.5278\n",
      "RMSE: 752.6269\n",
      "R² Score: 0.0550\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Predictions\n",
    "y_pred_cnn = cnn_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "evaluate_model(y_test, y_pred_cnn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Performance Evaluation**\n",
    "\n",
    "#### **1. Evaluation Metrics**\n",
    "The regression model was evaluated using **Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R² Score**.\n",
    "\n",
    "- **MAE:** `514.5278`  \n",
    "  - This means that, on average, the model's predictions deviate by approximately **514.53** units from the actual values.  \n",
    "\n",
    "- **RMSE:** `752.6269`  \n",
    "  - The RMSE indicates that the model's errors have a standard deviation of **752.63** units.  \n",
    "  - Since RMSE penalizes large errors more than MAE, it suggests the presence of some outliers or high-variance errors.\n",
    "\n",
    "- **R² Score:** `0.0550`  \n",
    "  - The R² score of **0.0550** indicates that the model explains only **5.5% of the variance** in the target variable.  \n",
    "  - A low R² value suggests that the model is **not capturing the underlying pattern well** and might be underfitting.\n",
    "\n",
    "\n",
    "#### **2. Key Observations**\n",
    "- The **slight improvement in R² Score** (from `0.0371` to `0.0550`) suggests **minor enhancement**, but the model **still performs poorly**.\n",
    "- High **MAE and RMSE values** indicate that the model **lacks accuracy** in predictions.\n",
    "- A **low R² Score** suggests that **features may not be strongly correlated with the target** or that **feature selection and engineering need improvement**.\n",
    "\n",
    "  \n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
